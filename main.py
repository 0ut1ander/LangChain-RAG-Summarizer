# -*- coding: utf-8 -*-
"""LangChain-demo_new.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qFhc_Hs9K-UlUxb5zXp7wyz66RcdzG0X

# ----------------- **DEMO** ---------------------
"""

# --- STEP 1: INSTALLATION ---

!pip install --upgrade langchain langchain-community langchain-huggingface langchain-openai chromadb transformers sentencepiece accelerate

import os
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline

# --- STEP 2: DEFINE THE MODELS (Hugging Face & UM-GPT Template) ---

# A. Local Embeddings (Hugging Face)
# This model turns the text into vectors for ChromaDB.

hf_embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-l6-v2")

# B. Local LLM (Hugging Face - for the free demo)
model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
hf_pipe = pipeline("text2text-generation", model=model, tokenizer=tokenizer, max_new_tokens=150)
llm_local = HuggingFacePipeline(pipeline=hf_pipe)

# C. UM-GPT API Template(I uesd the tokens in the project and dont have the tokens for the demo anymore)
# For the demo, I swap 'llm_local'.
from langchain_openai import ChatOpenAI
llm_umgpt = ChatOpenAI(
    model="gpt-4o",
    base_url="https://api.umgpt.umich.edu/v1",
    api_key="PASTE_TOKEN_HERE" # Placeholder
)

# --- STEP 3: DATA INGESTION (ChromaDB) ---

# 1. Create a sample document
text_content = """
Loren is a Master's student in Computer Science at the University of Michigan-Dearborn.
Loren is scheduled to graduate on December 31, 2025.
The University of Michigan-Dearborn excels in Engineering.
"""

# 2. Split the text into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
chunks = text_splitter.split_text(text_content)

# 3. Store in ChromaDB
vectorstore = Chroma.from_texts(chunks, hf_embeddings)
retriever = vectorstore.as_retriever()

# --- STEP 4: THE RAG CHAIN ---

# Function to join retrieved chunks into one string
def combine_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

prompt = ChatPromptTemplate.from_template(
    "Answer the question using ONLY the following context:\n{context}\n\nQuestion: {question}"
)

# Finalized RAG chain
chain = (
    {"context": retriever | combine_docs, "question": RunnablePassthrough()}
    | prompt
    | llm_local
    | StrOutputParser()
)

# --- STEP 5: RUN THE QUERY ---
query = "When is Loren's graduation date?"
result = chain.invoke(query)

print(f"QUERY: {query}")
print(f"ANSWER: {result}")

# --- STEP 5: RUN THE QUERY ---
query = "What university does Loren go to?"
result = chain.invoke(query)

print(f"QUERY: {query}")
print(f"ANSWER: {result}")