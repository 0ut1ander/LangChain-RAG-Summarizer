# -*- coding: utf-8 -*-
"""LangChain-demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13NnU5oHRdvu8LRnwS7SK22QSOtswfBFM

# ----------------- **DEMO** ---------------------
"""

!pip install --upgrade langchain langchain-community langchain-text-splitters chromadb transformers sentencepiece accelerate

from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import FakeEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# Use a small instruction-tuned model for demo
model_name = "google/flan-t5-small"  # 80MB, works fine on CPU, tried "tiiuae/falcon-7b-instruct" but it's too big and kept crashing
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Simple function to run the model
def local_llm(prompt: str) -> str:
    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(**inputs, max_new_tokens=150)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Wrap in Runnable style
class HuggingFaceLLM:
    def __init__(self, llm_func):
        self.llm_func = llm_func

    def _call(self, prompt: str, stop=None):
        return self.llm_func(prompt)

    @property
    def _llm_type(self):
        return "huggingface"

llm = HuggingFaceLLM(local_llm)

# Create sample.txt
with open("sample.txt", "w") as f:
    f.write(
        "I go to University of Michigan = Dearborn.\n"
        "It is a leading research university.\n"
        "LangChain helps me build this LLM applications.\n"
        "This is a demo for my LangChain project."
    )

# Load documents
loader = TextLoader("sample.txt")
documents = loader.load()

# Split into chunks (smaller for short text)
splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=5)
splits = splitter.split_documents(documents)

# Fake vectorstore + retriever
embeddings = FakeEmbeddings(size=10)
vectorstore = Chroma.from_documents(splits, embedding=embeddings)
retriever = vectorstore.as_retriever()

print(f"Sample file created and split into {len(splits)} chunks.")

# Define LLM Runnable (deterministic settings)
from langchain_community.llms import HuggingFacePipeline

llm_runnable = HuggingFacePipeline.from_model_id(
    model_id="google/flan-t5-small",  # text2text generation
    task="text2text-generation",
    device=-1,  # CPU
    model_kwargs={
        "do_sample": False,   # disables randomness
        "temperature": 0.0,   # deterministic
        "top_p": 1.0           # full probability mass
    }
)

from langchain_core.runnables import RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# Prompt instructing exact extraction
prompt = ChatPromptTemplate.from_template(
    "Answer the question using ONLY the context below. "
    "Do NOT summarize. Copy the answer exactly as it appears in the context.\n\n"
    "Context:\n{context}\n\nQuestion: {question}"
)

# Combine all document chunks
def combine_docs(docs):
    return "\n\n".join(d.page_content for d in docs)

# Build the chain
chain = (
    {"context": retriever | combine_docs, "question": RunnablePassthrough()}
    | prompt
    | llm_runnable
)

# Run query
query = "Where does the author go to university?"
result = chain.invoke(query)
print("Answer:", result)